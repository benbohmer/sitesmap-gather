{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cff431d-b7e1-413e-87a8-f59728f74482",
   "metadata": {},
   "source": [
    "# Sitesmap-gather\n",
    "Get a sites xml sitemap files and save locally as .csv files\n",
    "\n",
    "Note: I am learning Python here. It's my first thing. Don't laugh.\n",
    "\n",
    "Documentation / comments are intermittent, sporadic and anemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256fa338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import requests # for downloading sitemap files\n",
    "import os, glob # to check current directory, to combine csv files\n",
    "from lxml import etree\n",
    "from datetime import datetime # so we can get some time info for debugging and progress indication\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7832a54-1d49-46bc-8070-accb157cd431",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ToDo\n",
    "\n",
    "- [x] put all files into subdir with same name as domain\n",
    "- [x] add user agent to avoid refusal of download\n",
    "    - credits:\n",
    "    - https://www.shellhacks.com/python-requests-user-agent-web-scraping/\n",
    "- [x] add date to subdir name to avoid overwriting\n",
    "- [ ] auto check if online xml or local csv, so we don't have to specify that ourselves\n",
    "- [ ] auto check if sitemap index file or not\n",
    "- [x] merge csv files\n",
    "    - credits:\n",
    "    -  https://softhints.com/how-to-merge-multiple-csv-files-with-python/\n",
    "- [ ] delete .xml files\n",
    "- [ ] add UI\n",
    "    - [ ] user selectable directory to save csv's in\n",
    "- [ ] recursion - submit a list of sitemap index files and get everything within them\n",
    "- [ ] bug: UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1112525: character maps to <undefined>\n",
    "    - during https://www.tv2.no/sitemap/sitemap.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d8d91",
   "metadata": {},
   "source": [
    "# Defining Our Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b4d0c-8223-40eb-a2fc-ef63f0bfbe8e",
   "metadata": {},
   "source": [
    "## Download an xml sitemap file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sitemap-file\n",
    "# Modified to set user agent to a desktop browser, since some sites refuse to serve it's sitemaps otherwise\n",
    "\n",
    "def download_sitemap(url,fileToDownload, debug=None):\n",
    "\n",
    "    headers = {\n",
    "     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; rv:91.0) Gecko/20100101 Firefox/91.0' \n",
    "    }\n",
    "    \n",
    "    subDirToCreate = file_info(url,path=True) # maybe we need to create a subdir based on the domain name from the sitemap, checking if subdir exists is done by create_subdir\n",
    "    create_dir(subDirToCreate)\n",
    "    \n",
    "    r = requests.get(url, allow_redirects=True, headers=headers)\n",
    "    open(fileToDownload, 'wb').write(r.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec6259-db65-4452-8706-c5d17f1455c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read the XML from downloaded sitemap file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get XML from local sitemap-file\n",
    "def get_xml(fileToRead):\n",
    "    xml = BeautifulSoup(open(fileToRead).read())\n",
    "    return xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b37e63-fa59-48ab-8c37-4e4d0e5ae503",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test for sitemap index file or regular sitemap file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8491781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check whether the sitemap is an index sitemap, or regular sitemap\n",
    "# This seems slow to me, as it has xml.find_all, but it might be performant for all I know\n",
    "\n",
    "def get_sitemap_type(xml):\n",
    "    sitemapindex = xml.find_all('sitemapindex')\n",
    "    sitemap = xml.find_all('urlset')\n",
    "    \n",
    "    if sitemapindex:\n",
    "        return 'sitemapindex'\n",
    "    elif sitemap:\n",
    "        return 'urlset'\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ac0fb-53be-4137-a773-7df2f9adf46e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load XMl into padas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load XML into a pandas dataframe\n",
    "def sitemap_to_dataframe(xml, name=None, data=None, verbose=False):\n",
    "    df = pd.DataFrame(columns=['domain','sitemap_name','loc'])\n",
    "    \n",
    "    urls = xml.find_all(\"url\")\n",
    "    \n",
    "    for x in urls:\n",
    "\n",
    "        if xml.find(\"loc\"):\n",
    "            loc = x.findNext(\"loc\").text\n",
    "            parsed_uri = urlparse(loc)\n",
    "            domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "        else:\n",
    "            loc = ''\n",
    "            domain = ''\n",
    "\n",
    "#        if xml.find(\"changefreq\"):\n",
    "#            changefreq = x.findNext(\"changefreq\").text\n",
    "#        else:\n",
    "#            changefreq = ''\n",
    "\n",
    "#        if xml.find(\"priority\"):\n",
    "#            priority = x.findNext(\"priority\").text\n",
    "#        else:\n",
    "#            priority = ''\n",
    "\n",
    "        if name:\n",
    "            sitemap_name = name\n",
    "        else:\n",
    "            sitemap_name = ''\n",
    "              \n",
    "        row = {\n",
    "            'domain': domain,\n",
    "            'loc': loc,\n",
    "#            'changefreq': changefreq,\n",
    "#            'priority': priority,\n",
    "            'sitemap_name': sitemap_name,\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(row)\n",
    "\n",
    "        df = df.append(row, ignore_index=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3954b-e56a-4147-b757-3ca10704ee84",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load list of sitemaps\n",
    "Either from csv or xml index sitemap file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ef1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a list of sitemaps\n",
    "# either from csv or sitemap index\n",
    "#def get_sitemaplist(list_file):\n",
    "#    sitemapList = []\n",
    "#    df = pd.read_csv(list_file)\n",
    "#    sitemapList = df.values.tolist()\n",
    "#    return sitemapList\n",
    "\n",
    "\n",
    "def get_sitemaplist(listToGet, list_type=None):\n",
    "    if list_type == 'csv':\n",
    "        sitemapList = []\n",
    "        df = pd.read_csv(listToGet)\n",
    "        sitemapList = df.values.tolist()\n",
    "        return sitemapList\n",
    "    if list_type == 'xml':\n",
    "        url = listToGet\n",
    "        filename = file_info(listToGet,name=True)\n",
    "        filepath = file_info(listToGet,path=True)\n",
    "        filelocation = file_info(listToGet,location=True)\n",
    "        \n",
    "        download_sitemap(listToGet,filelocation)\n",
    "\n",
    "        sitemapList = get_child_sitemaps(get_xml(filelocation))\n",
    "        return sitemapList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827f4ba-db21-45d2-bc13-5747fd6a568f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get child sitemaps from index sitemap file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2160389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get child sitemaps from sitemap index file\n",
    "# takes xml\n",
    "# returns list\n",
    "\n",
    "def get_child_sitemaps(xml):\n",
    "    sitemaps = xml.find_all(\"sitemap\")\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for sitemap in sitemaps:\n",
    "        output.append(sitemap.findNext(\"loc\").text)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6860d-3f42-411d-83fc-e5ca0739e6c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## File Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5762cd5-a927-4a9d-908b-97cd0f896d69",
   "metadata": {},
   "source": [
    "### Get filename, filepath, filelocation, domain from the URL of a Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15a8bab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get filename, filepath or filelocation of sitemap url\n",
    "\n",
    "# adding subdir support\n",
    "def file_info(url,name=None, path=None, location=None, domain=None):\n",
    "    dateToday = str(date.today())\n",
    "    domain = urlparse(url).netloc\n",
    "    filename = url[url.rfind(\"/\")+1:len(url)] # extract just the filename from the URL    \n",
    "    #filepath = os.getcwd()+\"\\\\\"+domain+\"-\"+dateToday # get the current directory    \n",
    "    filepath = \"C:\\\\Users\\\\ben\\\\Downloads\\\\sitesmapgather\\\\\"+domain+\"-\"+dateToday\n",
    "    filelocation = filepath+\"\\\\\"+filename # construct a complete URI for the file, including a subdir named after the domain\n",
    "    \n",
    "    if name:\n",
    "        return filename\n",
    "    if path:\n",
    "        return filepath\n",
    "    if location:\n",
    "        return filelocation\n",
    "    if domain:\n",
    "        return domain\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeaf78f-c934-433f-b0c2-2e3ee645f6d6",
   "metadata": {},
   "source": [
    "### Create a directory\n",
    "but test if it exists first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f25240c-9964-4e74-b583-f7b1311ccb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subdirectory named after the domain\n",
    "# we also check for whether it exists\n",
    "# maybe this check should be in the main loop?\n",
    "\n",
    "def create_dir(subdirToCreate):\n",
    "#    subdirToCreateLocation = file_info(subdirToCreate,domain=True) # set subdir variable\n",
    "    if not os.path.isdir(subdirToCreate): # does the subdir not exist?\n",
    "        os.mkdir(subdirToCreate) # then create it\n",
    "        #print('created directory: '+subdirToCreate)\n",
    "    else: # if it does exist\n",
    "        print('directory exists: '+subdirToCreate) # do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798aa59f-0237-4ca9-ae73-65e1fb300af2",
   "metadata": {},
   "source": [
    "### Combine csv files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2d04b-3420-46a7-88bc-bf7be2898e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from\n",
    "#  https://softhints.com/how-to-merge-multiple-csv-files-with-python/\n",
    "\n",
    "def combine_csv(pathToCombine):\n",
    "\n",
    "    all_files = glob.glob(os.path.join(pathToCombine, \"*.csv\"))\n",
    "   \n",
    "    all_df = []\n",
    "    for f in all_files:\n",
    "        df = pd.read_csv(f, sep=',')\n",
    "        df['file'] = f.split('/')[-1]\n",
    "        all_df.append(df)\n",
    "        \n",
    "    merged_df = pd.concat(all_df, ignore_index=True, sort=True)\n",
    "    merged_df.to_csv(pathToCombine+\"\\\\\"+\"merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec3c7d-f35e-4fba-a2e0-0044e350e2ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Workhorse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74566aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### main workhorse\n",
    "### 1. takes url\n",
    "### 2. downloads file\n",
    "### 3. gets the xml from the file\n",
    "### 4. creates dataframe\n",
    "### 5. saves dataframe as csv\n",
    "\n",
    "def sitemap_to_csv(url,debug=False):\n",
    "\n",
    "    \n",
    "    filename = file_info(url,name=True)\n",
    "    filepath = file_info(url,path=True)\n",
    "    filelocation = file_info(url,location=True)\n",
    "    \n",
    "\n",
    "    if debug:\n",
    "        time_mainstart = datetime.now()\n",
    "        print('sitemap                : ' +filename)\n",
    "        print('start                  : ' +str(time_mainstart))\n",
    "\n",
    "    # call download_sitemap method with url and filename\n",
    "    if debug:\n",
    "        time_start = datetime.now()\n",
    "        print('    starting download      : '+str(time_start))\n",
    "    download_sitemap(url,filelocation) \n",
    "    if debug:\n",
    "        time_end = datetime.now()\n",
    "        print('    end download           : '+str(time_end-time_start))\n",
    "        time_start = datetime.now()\n",
    "        print('    start get xml          : '+str(time_start))    \n",
    "    # read the xml out of the local file\n",
    "    xml = get_xml(filelocation)\n",
    "    if debug:\n",
    "        time_end = datetime.now()\n",
    "        print('    end get xml            : '+str(time_end-time_start))\n",
    "        time_start = datetime.now()\n",
    "        print('    start create dataframe : '+str(time_start))    \n",
    "    thing = sitemap_to_dataframe(xml,name=filename)\n",
    "    if debug:\n",
    "        time_end = datetime.now()\n",
    "        print('    end create dataframe   : '+str(time_end-time_start))\n",
    "        time_start = datetime.now()\n",
    "        print('    start write csv        : '+str(time_start))    \n",
    "    thing.to_csv(filelocation+'.csv',index=False)\n",
    "    if debug:\n",
    "        time_end = datetime.now()\n",
    "        print('    end write csv files          : '+str(time_end-time_start))    \n",
    "        time_mainend = datetime.now()\n",
    "        print('end                        : '+str(time_mainend-time_mainstart))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722271ca",
   "metadata": {},
   "source": [
    "# Here begins the testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151e818",
   "metadata": {},
   "source": [
    "## Testing main methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e833a-a5bf-43bb-ade0-7ef06456efc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get a single sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d932e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# trying the main method - get a url and write csv\n",
    "url = 'https://www.elon.se/sitemap.xml' \n",
    "sitemap_to_csv(url,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b0c3b4-c50d-4f96-bc60-8bbbbcf9c5d3",
   "metadata": {},
   "source": [
    "### Combine csv files in a folder\n",
    "\"in a folder\" is a lie. It looks in a subfolder named after the domain of the sitemap URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8dce11-d951-42c5-a5d3-909060dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combine = file_info('https://www.elkjop.no/service-sitemap-site-elkjop-no-no-sitemap_index.xml',path=True)\n",
    "print(test_combine)\n",
    "combine_csv(test_combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb80e10-8c7c-4e7a-a9c7-11f7324f9489",
   "metadata": {},
   "source": [
    "### Main loop for a list of sitemap index files\n",
    "Provide a list of index sitemap files, and we loop through them.\n",
    "Useful for multinationals with several TLD's, or just getting a heap of different sites while you look out the window*\n",
    "\n",
    "*) Do that. Look out the window! Your eyes need the excercise, your circadian rythm will sync to the daylight, your sense of presence in the physical world wil solidify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8ea5d-d552-44f2-9aa3-dcdd44dd356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through a list of index files\n",
    "\n",
    "indexList = ['https://www.elkjop.no/sitemaps/OCNOELK.pdp.index.sitemap.xml',\n",
    "             'https://www.gigantti.fi/sitemaps/OCFIGIG.pdp.index.sitemap.xml',\n",
    "             'https://www.elgiganten.dk/sitemaps/OCDKELG.pdp.index.sitemap.xml',\n",
    "             'https://www.elgiganten.se/sitemaps/OCSEELG.pdp.index.sitemap.xml']\n",
    "#indexList = ['https://www.elkjop.no/service-sitemap-site-elkjop-no-no-sitemap_index.xml',\n",
    "#             'https://www.gigantti.fi/service-sitemap-site-gigantti-fi-fi-sitemap_index.xml',\n",
    "#             'https://www.elgiganten.dk/service-sitemap-site-elgiganten-dk-da-sitemap_index.xml',\n",
    "#             'https://www.elgiganten.se/service-sitemap-site-elgiganten-se-sv-sitemap_index.xml']\n",
    "\n",
    "#print(indexList)\n",
    "\n",
    "\n",
    "for y in indexList:\n",
    "    pathToCombine = file_info(y,path=True)\n",
    "    sitemapList = get_sitemaplist(y,'xml')\n",
    "    for x in sitemapList:\n",
    "        currentURL = str(x)\n",
    "        sitemap_to_csv(currentURL,debug=True)\n",
    "    \n",
    "    print('path to combine: '+pathToCombine)\n",
    "    combine_csv(pathToCombine) # when this inner loop is done, combine the csv files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c339a3e-f120-4cfd-831c-72121c868631",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Main loop for a list of sitemaps\n",
    "Either from local csv or online xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "123c982f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.tv2.no/sitemap/nyheter-0.xml', 'https://www.tv2.no/sitemap/nyheter-1.xml', 'https://www.tv2.no/sitemap/nyheter-2.xml', 'https://www.tv2.no/sitemap/nyheter-3.xml', 'https://www.tv2.no/sitemap/nyheter-4.xml', 'https://www.tv2.no/sitemap/nyheter-5.xml', 'https://www.tv2.no/sitemap/nyheter-6.xml', 'https://www.tv2.no/sitemap/nyheter-7.xml', 'https://www.tv2.no/sitemap/nyheter-8.xml', 'https://www.tv2.no/sitemap/nyheter-9.xml', 'https://www.tv2.no/sitemap/nyheter-10.xml', 'https://www.tv2.no/sitemap/nyheter-11.xml', 'https://www.tv2.no/sitemap/nyheter-12.xml', 'https://www.tv2.no/sitemap/nyheter-13.xml', 'https://www.tv2.no/sitemap/nyheter-14.xml', 'https://www.tv2.no/sitemap/nyheter-15.xml', 'https://www.tv2.no/sitemap/nyheter-16.xml', 'https://www.tv2.no/sitemap/nyheter-17.xml', 'https://www.tv2.no/sitemap/nyheter-18.xml', 'https://www.tv2.no/sitemap/nyheter-19.xml', 'https://www.tv2.no/sitemap/nyheter-20.xml', 'https://www.tv2.no/sitemap/nyheter-21.xml', 'https://www.tv2.no/sitemap/nyheter-22.xml', 'https://www.tv2.no/sitemap/nyheter-23.xml', 'https://www.tv2.no/sitemap/nyheter-24.xml', 'https://www.tv2.no/sitemap/nyheter-25.xml', 'https://www.tv2.no/sitemap/nyheter-26.xml', 'https://www.tv2.no/sitemap/nyheter-27.xml', 'https://www.tv2.no/sitemap/nyheter-28.xml', 'https://www.tv2.no/sitemap/nyheter-29.xml', 'https://www.tv2.no/sitemap/nyheter-30.xml', 'https://www.tv2.no/sitemap/nyheter-31.xml', 'https://www.tv2.no/sitemap/nyheter-32.xml', 'https://www.tv2.no/sitemap/nyheter-33.xml', 'https://www.tv2.no/sitemap/nyheter-34.xml', 'https://www.tv2.no/sitemap/nyheter-35.xml', 'https://www.tv2.no/sitemap/nyheter-36.xml', 'https://www.tv2.no/sitemap/nyheter-37.xml', 'https://www.tv2.no/sitemap/nyheter-38.xml', 'https://www.tv2.no/sitemap/nyheter-39.xml', 'https://www.tv2.no/sitemap/nyheter-40.xml', 'https://www.tv2.no/sitemap/nyheter-41.xml', 'https://www.tv2.no/sitemap/nyheter-42.xml', 'https://www.tv2.no/sitemap/nyheter-43.xml', 'https://www.tv2.no/sitemap/nyheter-44.xml', 'https://www.tv2.no/sitemap/nyheter-45.xml', 'https://www.tv2.no/sitemap/nyheter-46.xml', 'https://www.tv2.no/sitemap/nyheter-47.xml', 'https://www.tv2.no/sitemap/nyheter-48.xml', 'https://www.tv2.no/sitemap/nyheter-49.xml', 'https://www.tv2.no/sitemap/nyheter-50.xml', 'https://www.tv2.no/sitemap/nyheter-51.xml', 'https://www.tv2.no/sitemap/nyheter-52.xml', 'https://www.tv2.no/sitemap/nyheter-53.xml', 'https://www.tv2.no/sitemap/nyheter-54.xml', 'https://www.tv2.no/sitemap/nyheter-55.xml', 'https://www.tv2.no/sitemap/nyheter-56.xml', 'https://www.tv2.no/sitemap/nyheter-57.xml', 'https://www.tv2.no/sitemap/nyheter-58.xml', 'https://www.tv2.no/sitemap/nyheter-59.xml', 'https://www.tv2.no/sitemap/nyheter-60.xml', 'https://www.tv2.no/sitemap/nyheter-61.xml', 'https://www.tv2.no/sitemap/nyheter-62.xml', 'https://www.tv2.no/sitemap/nyheter-63.xml', 'https://www.tv2.no/sitemap/nyheter-64.xml', 'https://www.tv2.no/sitemap/nyheter-65.xml', 'https://www.tv2.no/sitemap/nyheter-66.xml', 'https://www.tv2.no/sitemap/nyheter-67.xml', 'https://www.tv2.no/sitemap/nyheter-68.xml', 'https://www.tv2.no/sitemap/nyheter-69.xml', 'https://www.tv2.no/sitemap/nyheter-70.xml', 'https://www.tv2.no/sitemap/nyheter-71.xml', 'https://www.tv2.no/sitemap/nyheter-72.xml', 'https://www.tv2.no/sitemap/nyheter-73.xml', 'https://www.tv2.no/sitemap/nyheter-74.xml', 'https://www.tv2.no/sitemap/nyheter-75.xml', 'https://www.tv2.no/sitemap/nyheter-76.xml', 'https://www.tv2.no/sitemap/nyheter-77.xml', 'https://www.tv2.no/sitemap/nyheter-78.xml', 'https://www.tv2.no/sitemap/nyheter-79.xml', 'https://www.tv2.no/sitemap/nyheter-80.xml', 'https://www.tv2.no/sitemap/nyheter-81.xml', 'https://www.tv2.no/sitemap/nyheter-82.xml', 'https://www.tv2.no/sitemap/nyheter-83.xml', 'https://www.tv2.no/sitemap/nyheter-84.xml', 'https://www.tv2.no/sitemap/nyheter-85.xml', 'https://www.tv2.no/sitemap/nyheter-86.xml', 'https://www.tv2.no/sitemap/nyheter-87.xml', 'https://www.tv2.no/sitemap/nyheter-88.xml', 'https://www.tv2.no/sitemap/nyheter-89.xml', 'https://www.tv2.no/sitemap/nyheter-90.xml', 'https://www.tv2.no/sitemap/nyheter-91.xml', 'https://www.tv2.no/sitemap/nyheter-92.xml', 'https://www.tv2.no/sitemap/nyheter-93.xml', 'https://www.tv2.no/sitemap/nyheter-94.xml', 'https://www.tv2.no/sitemap/nyheter-95.xml', 'https://www.tv2.no/sitemap/nyheter-96.xml', 'https://www.tv2.no/sitemap/nyheter-97.xml', 'https://www.tv2.no/sitemap/nyheter-98.xml', 'https://www.tv2.no/sitemap/nyheter-99.xml', 'https://www.tv2.no/sitemap/nyheter-100.xml', 'https://www.tv2.no/sitemap/nyheter-101.xml', 'https://www.tv2.no/sitemap/nyheter-102.xml', 'https://www.tv2.no/sitemap/nyheter-103.xml', 'https://www.tv2.no/sitemap/nyheter-104.xml', 'https://www.tv2.no/sitemap/sport-0.xml', 'https://www.tv2.no/sitemap/sport-1.xml', 'https://www.tv2.no/sitemap/sport-2.xml', 'https://www.tv2.no/sitemap/sport-3.xml', 'https://www.tv2.no/sitemap/sport-4.xml', 'https://www.tv2.no/sitemap/sport-5.xml', 'https://www.tv2.no/sitemap/sport-6.xml', 'https://www.tv2.no/sitemap/sport-7.xml', 'https://www.tv2.no/sitemap/sport-8.xml', 'https://www.tv2.no/sitemap/sport-9.xml', 'https://www.tv2.no/sitemap/sport-10.xml', 'https://www.tv2.no/sitemap/sport-11.xml', 'https://www.tv2.no/sitemap/sport-12.xml', 'https://www.tv2.no/sitemap/sport-13.xml', 'https://www.tv2.no/sitemap/sport-14.xml', 'https://www.tv2.no/sitemap/sport-15.xml', 'https://www.tv2.no/sitemap/sport-16.xml', 'https://www.tv2.no/sitemap/sport-17.xml', 'https://www.tv2.no/sitemap/sport-18.xml', 'https://www.tv2.no/sitemap/sport-19.xml', 'https://www.tv2.no/sitemap/sport-20.xml', 'https://www.tv2.no/sitemap/sport-21.xml', 'https://www.tv2.no/sitemap/sport-22.xml', 'https://www.tv2.no/sitemap/sport-23.xml', 'https://www.tv2.no/sitemap/sport-24.xml', 'https://www.tv2.no/sitemap/sport-25.xml', 'https://www.tv2.no/sitemap/sport-26.xml', 'https://www.tv2.no/sitemap/sport-27.xml', 'https://www.tv2.no/sitemap/sport-28.xml', 'https://www.tv2.no/sitemap/sport-29.xml', 'https://www.tv2.no/sitemap/sport-30.xml', 'https://www.tv2.no/sitemap/sport-31.xml', 'https://www.tv2.no/sitemap/sport-32.xml', 'https://www.tv2.no/sitemap/sport-33.xml', 'https://www.tv2.no/sitemap/sport-34.xml', 'https://www.tv2.no/sitemap/sport-35.xml', 'https://www.tv2.no/sitemap/sport-36.xml', 'https://www.tv2.no/sitemap/sport-37.xml', 'https://www.tv2.no/sitemap/sport-38.xml', 'https://www.tv2.no/sitemap/sport-39.xml', 'https://www.tv2.no/sitemap/sport-40.xml', 'https://www.tv2.no/sitemap/sport-41.xml', 'https://www.tv2.no/sitemap/sport-42.xml', 'https://www.tv2.no/sitemap/sport-43.xml', 'https://www.tv2.no/sitemap/sport-44.xml', 'https://www.tv2.no/sitemap/sport-45.xml', 'https://www.tv2.no/sitemap/sport-46.xml', 'https://www.tv2.no/sitemap/sport-47.xml', 'https://www.tv2.no/sitemap/sport-48.xml', 'https://www.tv2.no/sitemap/sport-49.xml', 'https://www.tv2.no/sitemap/sport-50.xml', 'https://www.tv2.no/sitemap/sport-51.xml', 'https://www.tv2.no/sitemap/sport-52.xml', 'https://www.tv2.no/sitemap/sport-53.xml', 'https://www.tv2.no/sitemap/underholdning-0.xml', 'https://www.tv2.no/sitemap/underholdning-1.xml', 'https://www.tv2.no/sitemap/underholdning-2.xml', 'https://www.tv2.no/sitemap/underholdning-3.xml', 'https://www.tv2.no/sitemap/underholdning-4.xml', 'https://www.tv2.no/sitemap/underholdning-5.xml', 'https://www.tv2.no/sitemap/underholdning-6.xml', 'https://www.tv2.no/sitemap/underholdning-7.xml', 'https://www.tv2.no/sitemap/underholdning-8.xml', 'https://www.tv2.no/sitemap/underholdning-9.xml', 'https://www.tv2.no/sitemap/underholdning-10.xml', 'https://www.tv2.no/sitemap/underholdning-11.xml', 'https://www.tv2.no/sitemap/underholdning-12.xml', 'https://www.tv2.no/sitemap/underholdning-13.xml', 'https://www.tv2.no/sitemap/underholdning-14.xml', 'https://www.tv2.no/sitemap/underholdning-15.xml', 'https://www.tv2.no/sitemap/underholdning-16.xml', 'https://www.tv2.no/sitemap/underholdning-17.xml', 'https://www.tv2.no/sitemap/underholdning-18.xml', 'https://www.tv2.no/sitemap/underholdning-19.xml', 'https://www.tv2.no/sitemap/underholdning-20.xml', 'https://www.tv2.no/sitemap/underholdning-21.xml', 'https://www.tv2.no/sitemap/underholdning-22.xml', 'https://www.tv2.no/sitemap/underholdning-23.xml', 'https://www.tv2.no/sitemap/underholdning-24.xml', 'https://www.tv2.no/sitemap/storm-0.xml', 'https://www.tv2.no/sitemap/storm-1.xml', 'https://www.tv2.no/sitemap/storm-2.xml', 'https://www.tv2.no/sitemap/broom-0.xml', 'https://www.tv2.no/sitemap/broom-1.xml', 'https://www.tv2.no/sitemap/broom-2.xml', 'https://www.tv2.no/sitemap/broom-3.xml', 'https://www.tv2.no/sitemap/broom-4.xml', 'https://www.tv2.no/sitemap/broom-5.xml', 'https://www.tv2.no/sitemap/broom-6.xml', 'https://www.tv2.no/sitemap/broom-7.xml', 'https://www.tv2.no/sitemap/broom-8.xml']\n",
      "sitemap                : nyheter-0.xml\n",
      "start                  : 2022-04-11 20:21:23.153022\n",
      "    starting download      : 2022-04-11 20:21:23.153022\n",
      "directory exists: C:\\Users\\ben\\Downloads\\sitesmapgather\\www.tv2.no-2022-04-11\n",
      "    end download           : 0:00:00.490489\n",
      "    start get xml          : 2022-04-11 20:21:23.643511\n",
      "    end get xml            : 0:00:02.605913\n",
      "    start create dataframe : 2022-04-11 20:21:26.250501\n",
      "    end create dataframe   : 0:00:05.992477\n",
      "    start write csv        : 2022-04-11 20:21:32.242978\n",
      "    end write csv files          : 0:00:00.013955\n",
      "end                        : 0:00:09.103911\n",
      "sitemap                : nyheter-1.xml\n",
      "start                  : 2022-04-11 20:21:32.256933\n",
      "    starting download      : 2022-04-11 20:21:32.256933\n",
      "directory exists: C:\\Users\\ben\\Downloads\\sitesmapgather\\www.tv2.no-2022-04-11\n",
      "    end download           : 0:00:00.534324\n",
      "    start get xml          : 2022-04-11 20:21:32.791376\n",
      "    end get xml            : 0:00:02.481478\n",
      "    start create dataframe : 2022-04-11 20:21:35.272854\n",
      "    end create dataframe   : 0:00:06.055310\n",
      "    start write csv        : 2022-04-11 20:21:41.328164\n",
      "    end write csv files          : 0:00:00.009971\n",
      "end                        : 0:00:09.081202\n",
      "sitemap                : nyheter-2.xml\n",
      "start                  : 2022-04-11 20:21:41.338135\n",
      "    starting download      : 2022-04-11 20:21:41.338135\n",
      "directory exists: C:\\Users\\ben\\Downloads\\sitesmapgather\\www.tv2.no-2022-04-11\n",
      "    end download           : 0:00:00.695138\n",
      "    start get xml          : 2022-04-11 20:21:42.033273\n",
      "    end get xml            : 0:00:02.625032\n",
      "    start create dataframe : 2022-04-11 20:21:44.658404\n",
      "    end create dataframe   : 0:00:06.608969\n",
      "    start write csv        : 2022-04-11 20:21:51.267373\n",
      "    end write csv files          : 0:00:00.009431\n",
      "end                        : 0:00:09.938669\n",
      "sitemap                : nyheter-3.xml\n",
      "start                  : 2022-04-11 20:21:51.276804\n",
      "    starting download      : 2022-04-11 20:21:51.276804\n",
      "directory exists: C:\\Users\\ben\\Downloads\\sitesmapgather\\www.tv2.no-2022-04-11\n",
      "    end download           : 0:00:00.530344\n",
      "    start get xml          : 2022-04-11 20:21:51.807148\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 1112525: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36924/450205380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msitemapList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcurrentURL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msitemap_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrentURL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36924/406774632.py\u001b[0m in \u001b[0;36msitemap_to_csv\u001b[1;34m(url, debug)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'    start get xml          : '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# read the xml out of the local file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mxml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_xml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilelocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mtime_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36924/4141699812.py\u001b[0m in \u001b[0;36mget_xml\u001b[1;34m(fileToRead)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get XML from local sitemap-file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_xml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileToRead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mxml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileToRead\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1112525: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# doing a loop through a sitemap list csv\n",
    "\n",
    "sitemapList = get_sitemaplist('https://www.tv2.no/sitemap/sitemap.xml','xml')\n",
    "print(sitemapList)\n",
    "\n",
    "for x in sitemapList:\n",
    "    currentURL = str(x)\n",
    "    sitemap_to_csv(currentURL,debug=True)\n",
    "    \n",
    "\n",
    "# url = 'https://www.gigantti.fi/service-sitemap-site-gigantti-fi-fi-sitemap_vc.xml' \n",
    "# sitemap_to_csv(url,debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e6b68",
   "metadata": {},
   "source": [
    "## Testing all the smaller cogs in the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56385553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# check the get_sitemaplist method\n",
    "\n",
    "#sitemap_to_csv('https://arstechnica.com/sitemap.xml',debug=True)\n",
    "#get_sitemaplist('sitemap_list.csv','csv')\n",
    "get_sitemaplist('https://arstechnica.com/sitemap.xml','xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# test the get_child_sitemap method\n",
    "\n",
    "url = 'https://www.gigantti.fi/service-sitemap-site-gigantti-fi-fi-sitemap_index.xml'\n",
    "\n",
    "filename = file_info(url,name=True)\n",
    "filepath = file_info(url,path=True)\n",
    "filelocation = file_info(url,location=True)\n",
    "\n",
    "download_sitemap(url,filelocation)\n",
    "\n",
    "get_child_sitemaps(get_xml(filelocation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a29dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "sitemap_to_csv('https://www.elkjop.no/service-sitemap-site-elkjop-no-no-sitemap_index.xml',debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eecd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "testname = file_info('https://arstechnica.com/sitemap.xml',name=True)\n",
    "testpath = file_info('https://arstechnica.com/sitemap.xml',path=True)\n",
    "testloc = file_info('https://arstechnica.com/sitemap.xml',location=True)\n",
    "testdom = file_info('https://arstechnica.com/sitemap.xml',domain=True)\n",
    "\n",
    "print(testname)\n",
    "print(type(testname))\n",
    "print(testpath)\n",
    "print(type(testpath))\n",
    "print(testloc)\n",
    "print(type(testloc))\n",
    "print(testdom)\n",
    "print(type(testdom))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b78163-d644-485e-9e74-a7974fa6b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date.today())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
